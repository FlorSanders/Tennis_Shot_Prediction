{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path_to_model_directory = '../model'\n",
    "\n",
    "# Add this path to sys.path\n",
    "if path_to_model_directory not in sys.path:\n",
    "    sys.path.append(path_to_model_directory)\n",
    "\n",
    "# Now you can import your class\n",
    "from PreTrainer import PreTrainer\n",
    "from data import TennisDataset\n",
    "from model_builder import build_tennis_embedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model based on config file: \n",
    "config_file = '/home/tawab/e6691-2024spring-project-TECO-as7092-gyt2107-fps2116/src/model/configs/default.yaml'\n",
    "model = build_tennis_embedder(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do for loading data\n",
    "- What do we want to do with the values being skipped? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping V006_0068: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V006_0179: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V007_0183: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V007_0184: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V008_0003: Data file not found.\n",
      "Skipping V008_0056: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V008_0156: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_0017: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_0924: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_0947: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_0948: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1045: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1281: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1282: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1542: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1553: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1734: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1742: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1860: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Train Dataset Loaded Successfully\n",
      "Skipping V006_0066: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V006_0178: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1046: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Skipping V009_1639: Incorrect dimensions - positions_2d (0, 2), poses_3d (0,)\n",
      "Val Dataset Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Load Train and Val Dataset\n",
    "train_path = '/home/florsanders/adl_ai_tennis_coach/data/tenniset/shot_labels/train'\n",
    "val_path = '/home/florsanders/adl_ai_tennis_coach/data/tenniset/shot_labels/val'\n",
    "train_dataset = TennisDataset(labels_path=train_path)\n",
    "print('Train Dataset Loaded Successfully')\n",
    "val_dataset = TennisDataset(labels_path=val_path)\n",
    "print('Val Dataset Loaded Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 2248\n",
      "Validation Dataset Length: 565\n",
      "Pose3d Shape: (24, 17, 3)\n",
      "Position2d Shape: (24, 2)\n",
      "Target Shape: (24, 17, 3)\n",
      "Number of Graphs:  24\n",
      "Pose Graph Data Shapes (x & edge_index):  Data(x=[17, 3], edge_index=[2, 38])\n",
      "Number of features: 3\n",
      "Number of nodes: 17\n",
      "Number of edges: 38\n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset Length:', len(train_dataset))\n",
    "print('Validation Dataset Length:', len(val_dataset))\n",
    "\n",
    "# Get pose3d, position2d, and pose graph \n",
    "pose3d, position2d, pose_graph, target = train_dataset[2]\n",
    "print('Pose3d Shape:', pose3d.shape)\n",
    "print('Position2d Shape:', position2d.shape)\n",
    "print('Target Shape:', target.shape)\n",
    "print('Number of Graphs: ', len(pose_graph))\n",
    "print('Pose Graph Data Shapes (x & edge_index): ' ,pose_graph[0])\n",
    "print(f'Number of features: {pose_graph[0].num_features}')\n",
    "print(f'Number of nodes: {pose_graph[0].num_nodes}')\n",
    "print(f'Number of edges: {pose_graph[0].num_edges}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PreTrainer\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do with the collate function \n",
    "- How do we want to handle np.object_ instances of pose3d and position2d? --> At the moment they are being ignored\n",
    "- How do we want to do padding of sequence graph to same length? --> At the moment the last instance is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch_geometric.data import Batch, Data\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def pad_graphs(graphs, max_frames):\n",
    "    padded_graphs = []\n",
    "    for graph_list in graphs:\n",
    "        num_graphs = len(graph_list)\n",
    "        if num_graphs < max_frames:\n",
    "            last_graph = graph_list[-1]\n",
    "            additional_graphs = [last_graph] * (max_frames - num_graphs)  # Replicate last graph\n",
    "            padded_graph_list = graph_list + additional_graphs\n",
    "        else:\n",
    "            padded_graph_list = graph_list[:max_frames]\n",
    "        \n",
    "        padded_graphs.append(padded_graph_list)\n",
    "        \n",
    "    return padded_graphs\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    pose3d = []\n",
    "    position2d = []\n",
    "    targets = []\n",
    "    all_graphs = []\n",
    "    graph_counts = []  # To count graphs per item in the batch\n",
    "\n",
    "\n",
    "    for item in batch:\n",
    "        pose3d_item, position2d_item, pose_graph_items, target_item = item\n",
    "        \n",
    "        # Check for None values and correct shapes\n",
    "        if pose3d_item is not None and position2d_item is not None and pose_graph_items is not None: \n",
    "            if pose3d_item.dtype != np.object_ and position2d_item.dtype != np.object_ and len(pose_graph_items) > 0:\n",
    "                \n",
    "                sequence_graphs = pose_graph_items\n",
    "\n",
    "                # Ensure numpy arrays are of type float32, convert object arrays if necessary\n",
    "                # if pose3d_item.dtype == np.object_:\n",
    "                #     print('NP Object!!')\n",
    "                #     print(\"Pose3d: \", pose3d_item)\n",
    "                #     pose3d_item = np.vstack(pose3d_item).astype(np.float32)\n",
    "                #     target_item = np.vstack(target_item).astype(np.float32)\n",
    "                # if position2d_item.dtype == np.object_:\n",
    "                #     print('NP Object!!')\n",
    "                #     print(\"Position2d: \", position2d_item)\n",
    "                #     position2d_item = np.vstack(position2d_item).astype(np.float32)\n",
    "                    \n",
    "                if pose3d_item.ndim == 3 and position2d_item.ndim == 2:  # Ensure the correct dimensionality\n",
    "                    \n",
    "                    graph_count = 0\n",
    "                    if isinstance(sequence_graphs, list):\n",
    "                        all_graphs.append(sequence_graphs)\n",
    "                        graph_count = len(sequence_graphs)  # Count graphs for this item\n",
    "                    else:\n",
    "                        print(\"Skipping a graph item due to incorrect type.\")\n",
    "                    graph_counts.append(graph_count)   \n",
    "\n",
    "                    pose3d.append(torch.tensor(pose3d_item, dtype=torch.float32))\n",
    "                    position2d.append(torch.tensor(position2d_item, dtype=torch.float32))\n",
    "                    targets.append(torch.tensor(target_item, dtype=torch.float32))\n",
    "                # else:\n",
    "                #     print(f\"Skipping due to incorrect dimensions - Pose3D: {pose3d_item.shape}, Position2D: {position2d_item.shape}\")\n",
    "            # else: \n",
    "            #     print(\"Skipping a batch item due to object dtype or graph being empty.\")\n",
    "        # else:\n",
    "        #     print(\"Skipping a batch item due to None values.\")\n",
    "\n",
    "    #print(\"Graphs per item in batch:\", graph_counts)  # For Debugging\n",
    "\n",
    "    # Pad pose3d and position2d sequences if not empty\n",
    "    pose3d_padded = pad_sequence(pose3d, batch_first=True) if pose3d else torch.Tensor()\n",
    "    position2d_padded = pad_sequence(position2d, batch_first=True) if position2d else torch.Tensor()\n",
    "    targets_padded = pad_sequence(targets, batch_first=True) if targets else torch.Tensor()\n",
    "\n",
    "    #print(\"Number of Graphs:\", len(all_graphs))\n",
    "    \n",
    "    # Create a list of Batch objects for each item in the batch\n",
    "    max_frames = max(len(graphs) for graphs in all_graphs)  # Maximum number of frames in the batch\n",
    "    #print(\"Max Frames:\", max_frames)\n",
    "    if len(all_graphs) > 0:\n",
    "        all_graphs = pad_graphs(all_graphs, max_frames)\n",
    "        batched_graphs = [Batch.from_data_list(graph_list) for graph_list in all_graphs]\n",
    "    else:\n",
    "        batched_graphs = []\n",
    "\n",
    "    return pose3d_padded, position2d_padded, batched_graphs, targets_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose3d Shape: torch.Size([32, 67, 17, 3])\n",
      "Position2d Shape: torch.Size([32, 67, 2])\n",
      "Target Shape: torch.Size([32, 67, 17, 3])\n",
      "Number of Sequence Graphs in Batch: 32\n",
      "Sequence Graph Data Shapes (x & edge_index):  DataBatch(x=[1139, 3], edge_index=[2, 2546], batch=[1139], ptr=[68])\n",
      "Number of Graphs in First Sequence: 67\n"
     ]
    }
   ],
   "source": [
    "for pose3d, position2d, pose_graph, target in train_loader:\n",
    "    x = pose3d\n",
    "    y = position2d\n",
    "    z = pose_graph\n",
    "    print('Pose3d Shape:', pose3d.shape)\n",
    "    print('Position2d Shape:', position2d.shape)\n",
    "    print('Target Shape:', target.shape)\n",
    "    print('Number of Sequence Graphs in Batch:', len(pose_graph))\n",
    "    print('Sequence Graph Data Shapes (x & edge_index): ' ,pose_graph[0])\n",
    "    print('Number of Graphs in First Sequence:', pose_graph[3].num_graphs)\n",
    "    x = pose_graph[0].batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Trainer Setup\n",
    "trainer = PreTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.519310984812992\n",
      "Test Loss: 3.9174769984351263\n",
      "Epoch 2, Loss: 3.4493121865769507\n",
      "Test Loss: 1.9674275981055365\n",
      "Epoch 3, Loss: 0.9792592051163526\n",
      "Test Loss: 0.7634573198027081\n",
      "Epoch 4, Loss: 0.7425360855921893\n",
      "Test Loss: 0.6903862721390195\n",
      "Epoch 5, Loss: 0.7162655608754762\n",
      "Test Loss: 0.6654880245526632\n",
      "Epoch 6, Loss: 0.6936964623525109\n",
      "Test Loss: 0.7403709126843346\n",
      "Epoch 7, Loss: 0.6784240993815409\n",
      "Test Loss: 0.6443121847179201\n",
      "Epoch 8, Loss: 0.6514911639018798\n",
      "Test Loss: 0.6629044148657057\n",
      "Epoch 9, Loss: 0.653844780065644\n",
      "Test Loss: 0.6156553261809878\n",
      "Epoch 10, Loss: 0.6356361739232507\n",
      "Test Loss: 0.6150351448191537\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
