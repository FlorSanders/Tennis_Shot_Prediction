{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 1 - Split tennis matches into point segments\n",
    "\n",
    "Starting from the full match videos and the annotations, we split them into shorter segments containing a single point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "from __init__ import data_path\n",
    "# Choose dataset\n",
    "dataset = \"tenniset\"\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "videos_path = os.path.join(dataset_path, \"videos\")\n",
    "annotations_path = os.path.join(dataset_path, \"annotations\")\n",
    "\n",
    "# Read videos\n",
    "videos = sorted(os.listdir(videos_path))\n",
    "print(\"___VIDEOS___\")\n",
    "for video in videos: print(video)\n",
    "\n",
    "# Determine where to save segments\n",
    "segments_path = os.path.join(dataset_path, \"segments\")\n",
    "os.makedirs(segments_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_video(\n",
    "    video, \n",
    "    segment_by=\"Point\", \n",
    "    videos_path=videos_path, \n",
    "    annotations_path=annotations_path, \n",
    "    segments_path=segments_path,\n",
    "    overwrite=False,\n",
    "):\n",
    "    # Video & annotation path\n",
    "    video_name, video_ext = os.path.splitext(video)\n",
    "    video_path = os.path.join(videos_path, video)\n",
    "    annotation_path = os.path.join(annotations_path, video.replace(video_ext, \".json\"))\n",
    "\n",
    "    # Load annotation\n",
    "    with open(annotation_path) as annotation_file:\n",
    "        annotation = json.load(annotation_file)\n",
    "\n",
    "    # Load video\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    frame = 0\n",
    "\n",
    "    # Get resolution & framerate from capture\n",
    "    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Perform segmentation\n",
    "    segments = annotation[\"classes\"][segment_by]\n",
    "    for index, segment in tqdm(enumerate(segments)):\n",
    "        # Determine path to save current segment\n",
    "        segment_nr = f\"{index}\".zfill(4)\n",
    "        segment_name = f\"{video_name}_{segment_nr}\"\n",
    "        segment_path = os.path.join(segments_path, f\"{segment_name}.mp4\")\n",
    "\n",
    "        # Don't overwrite if not needed\n",
    "        if os.path.exists(segment_path) and not overwrite:\n",
    "            continue\n",
    "\n",
    "        # Open video writer\n",
    "        writer = cv2.VideoWriter(segment_path, cv2.VideoWriter_fourcc(*\"avc1\"), fps, (frame_width, frame_height))\n",
    "\n",
    "        # segment points\n",
    "        start, end = int(segment[\"start\"]), int(segment[\"end\"])\n",
    "\n",
    "        # Fast forward to start of segment\n",
    "        frame = start\n",
    "        capture.set(1, frame)\n",
    "        \n",
    "        # Save frames to segment\n",
    "        while_safety = 0\n",
    "        max_while_safety = 500\n",
    "        while frame < end:\n",
    "            # Read frame\n",
    "            ret, img = capture.read()\n",
    "\n",
    "            # Sometimes OpenCV reads None's during a video, in which case we want to just skip\n",
    "            assert while_safety < max_while_safety, f\"ERROR, cv2 read {max_while_safety} Nones\"\n",
    "            if ret == 0 or img is None: \n",
    "                while_safety += 1\n",
    "                continue \n",
    "            while_safety = 0\n",
    "\n",
    "            # Write frame\n",
    "            writer.write(img)\n",
    "\n",
    "            # Increase frame counter\n",
    "            frame += 1\n",
    "        \n",
    "        # Release writer\n",
    "        writer.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_video(videos[0], overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in videos:\n",
    "    print(f\"Segmenting video {video}\")\n",
    "    segment_video(video, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_files = [f for f in os.listdir(segments_path) if f.endswith('.mp4')]\n",
    "n_segments = len(segment_files)\n",
    "total_frames = 0\n",
    "total_duration = 0\n",
    "for segment in segment_files:\n",
    "    segment_path = os.path.join(segments_path, segment)\n",
    "    # Give the duration and number of frames of the video\n",
    "    data = cv2.VideoCapture(segment_path) \n",
    "    frames = data.get(cv2.CAP_PROP_FRAME_COUNT) \n",
    "    fps = data.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # calculate duration of the video \n",
    "    duration = frames / fps\n",
    "\n",
    "    # Add to total\n",
    "    total_frames += frames\n",
    "    total_duration += duration\n",
    "\n",
    "print(f\"Total number of videos: {n_segments}\")\n",
    "print(f\"Total number of frames: {total_frames}\")\n",
    "print(f\"Total duration: {total_duration / 60 / 60:.2f} hours\")\n",
    "print(f\"Average duration per video: {total_duration / n_segments:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Court, Player & Ball Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the pre-trained models provided by the tennis-project repository:\n",
    "- [Ball Detection](https://drive.google.com/file/d/1XEYZ4myUN7QT-NeBYJI0xteLsvs-ZAOl/view): Save as `models/data/ball_detection/model.pt`\n",
    "- [Court Detection](https://drive.google.com/file/d/1f-Co64ehgq4uddcQm1aFBDtbnyZhQvgG/view): Save as `models/data/court_detection/model.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "from __init__ import data_path, models_path\n",
    "from data_utils import read_segment_frames, read_segment_labels, visualize_frame_annotations, visualize_segment_labels\n",
    "\n",
    "# Choose dataset\n",
    "dataset = \"tenniset\"\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "segments_path = os.path.join(dataset_path, \"segments\")\n",
    "labels_path = os.path.join(dataset_path, \"labels\")\n",
    "os.makedirs(labels_path, exist_ok=True)\n",
    "\n",
    "# Model paths\n",
    "model_ball_detection_path = os.path.join(models_path, \"ball_detection\", \"model.pt\")\n",
    "model_court_detection_path = os.path.join(models_path, \"court_detection\", \"model.pt\")\n",
    "assert os.path.exists(model_ball_detection_path), \"Please download model\"\n",
    "assert os.path.exists(model_court_detection_path), \"Please download model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tennis-project objects\n",
    "cwd = os.getcwd()\n",
    "tennis_project_path = os.path.abspath(os.path.join(cwd, \"tennis-project\"))\n",
    "sys.path.append(tennis_project_path)\n",
    "from court_detection_net import CourtDetectorNet\n",
    "from court_reference import CourtReference\n",
    "from bounce_detector import BounceDetector\n",
    "from person_detector import PersonDetector\n",
    "from ball_detector import BallDetector\n",
    "from utils import scene_detect\n",
    "from main import read_video\n",
    "from main import get_court_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read segment files\n",
    "segment_files = [file for file in sorted(os.listdir(segments_path)) if os.path.splitext(file)[-1] == \".mp4\"]\n",
    "print(f\"Number of segment files: {len(segment_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_bboxes(court_points, person_candidates, is_btm, frame_width=1280, frame_height=720):\n",
    "    # Setup\n",
    "    bbox = [None]*4\n",
    "    court_outline = np.array(kps_court[i][:4])\n",
    "    \n",
    "    # Make predictions\n",
    "    if len(person_candidates) == 0:\n",
    "        pass\n",
    "    elif len(person_candidates) == 1:\n",
    "        bbox = person_candidates[0][0]\n",
    "    else:\n",
    "        # Need to filter\n",
    "        if np.any(court_outline.reshape(-1) == None):\n",
    "            # We can't base our choice on the court lines... make guess\n",
    "            print(\"WARNING: Court points invalid - guessing\")\n",
    "            x_c = frame_width // 2\n",
    "            y_c = frame_height if is_btm else 0\n",
    "        else:\n",
    "            (x_tl, y_tl), (x_tr, y_tr), (x_bl, y_bl), (x_br, y_br) = court_outline\n",
    "            x_c = (x_bl + x_br) / 2 if is_btm else (x_tl + x_tr) / 2 \n",
    "            y_c = (y_bl + y_br) / 2 if is_btm else (y_tl + y_tr) / 2\n",
    "        distances = [(x_player - x_c)**2 + (y_player - y_c)**2 for _, (x_player, y_player) in person_candidates]\n",
    "        choice_index = np.argmin(distances)\n",
    "        bbox = person_candidates[choice_index][0]\n",
    "    \n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(\n",
    "    segment_path, \n",
    "    labels_path = labels_path, \n",
    "    verbose = True,\n",
    "    overwrite=False,\n",
    "):\n",
    "    # Start\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if segment already processed (by last saved file)\n",
    "    segment_dir, segment_filename = os.path.split(segment_path)\n",
    "    segment_name, segment_ext = os.path.splitext(segment_filename)\n",
    "    if os.path.exists(os.path.join(labels_path, f\"{segment_name}_ball.npy\")) and not overwrite:\n",
    "        #if verbose: print(f\"Segment {segment_name} already processed\")\n",
    "        return\n",
    "    if verbose: print(f\"Processing segment: {segment_name}\")\n",
    "\n",
    "    # Read video\n",
    "    if verbose: print(\"Reading video\")\n",
    "    frames, fps =  read_video(segment_path)\n",
    "    frame_height, frame_width, _ = frames[0].shape\n",
    "    video_time = time.time()\n",
    "    if verbose: print(f\"Done reading {len(frames)} video frames ({video_time - start_time} s)\")\n",
    "\n",
    "    # Load models\n",
    "    if verbose: print(\"Loading models\")\n",
    "    ball_detector = BallDetector(model_ball_detection_path, device)\n",
    "    court_detector = CourtDetectorNet(model_court_detection_path, device)\n",
    "    person_detector = PersonDetector(device)\n",
    "    models_time = time.time()\n",
    "    if verbose: print(f\"Done loading models ({models_time - video_time} s)\")\n",
    "\n",
    "    # Detect court \n",
    "    if verbose: print(\"Detecting court\")\n",
    "    homography_matrices, kps_court = court_detector.infer_model(frames)\n",
    "    court_time = time.time()\n",
    "    if verbose: print(f\"Done detecting court ({court_time - models_time} s)\")\n",
    "\n",
    "    # Detect whether frames are valid (i.e. court is detected)\n",
    "    frames_is_valid = np.array([not detection is None for detection in kps_court])\n",
    "    frames = [frame for frame, frame_is_valid in zip(frames, frames_is_valid) if frame_is_valid]\n",
    "    homography_matrices = [matrix for matrix, frame_is_valid in zip(homography_matrices, frames_is_valid) if frame_is_valid]\n",
    "    kps_court = [points for points, frame_is_valid in zip(kps_court, frames_is_valid) if frame_is_valid]\n",
    "\n",
    "    # Detect people\n",
    "    if verbose: print(\"Detecting people\")\n",
    "    persons_top, persons_bottom = person_detector.track_players(frames, homography_matrices, filter_players=False)\n",
    "    people_time = time.time()\n",
    "    if verbose: print(f\"Done detecting people ({people_time - court_time} s)\")\n",
    "\n",
    "    # Detect ball trajectory\n",
    "    if verbose: print(\"Detecting ball trajectory\")\n",
    "    ball_track = ball_detector.infer_model(frames)\n",
    "    ball_time = time.time()\n",
    "    if verbose: print(f\"Done detecting ball trajectory ({ball_time - people_time} s)\")\n",
    "\n",
    "    # Export labels\n",
    "    if verbose: print(\"Saving results\")\n",
    "    # Save frame validities\n",
    "    frames_is_valid_file = os.path.join(labels_path, f\"{segment_name}_frame_validity.npy\")\n",
    "    np.save(frames_is_valid_file, frames_is_valid)\n",
    "    # Save court\n",
    "    court_file = os.path.join(labels_path, f\"{segment_name}_court.npy\")\n",
    "    court_sequence = np.squeeze(np.array(kps_court))\n",
    "    np.save(court_file, court_sequence)\n",
    "    # Save players\n",
    "    for player_name, player_sequence in zip([\"btm\", \"top\"], [persons_bottom, persons_top]):\n",
    "        player_file = os.path.join(labels_path, f\"{segment_name}_player_{player_name}_bbox.npy\")\n",
    "        player_sequence = np.array([get_player_bboxes(kps_court[i], person_candidates, not is_top, frame_width=frame_width, frame_height=frame_height) for i, person_candidates in enumerate(player_sequence)])\n",
    "        np.save(player_file, player_sequence)\n",
    "    # Save ball trajectory\n",
    "    ball_file = os.path.join(labels_path, f\"{segment_name}_ball.npy\")\n",
    "    ball_sequence = np.array(ball_track)\n",
    "    np.save(ball_file, ball_sequence)\n",
    "    save_time = time.time()\n",
    "    if verbose: print(f\"Done detecting ball trajectory ({save_time - ball_time} s)\")\n",
    "    if verbose: print(f\"Total time: {save_time - start_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, segment_file in enumerate(segment_files):\n",
    "    process_segment(os.path.join(segments_path, segment_file), overwrite=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompute boxes!!!\n",
    "\n",
    "Bounding boxes selected the wrong player too often - recomputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homography import get_trans_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_bboxes(\n",
    "    segment_path, \n",
    "    labels_path = labels_path, \n",
    "    verbose=True,\n",
    "    overwrite=False,\n",
    "):\n",
    "    # Start\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if segment already processed (by last saved file)\n",
    "    segment_dir, segment_filename = os.path.split(segment_path)\n",
    "    segment_name, segment_ext = os.path.splitext(segment_filename)\n",
    "    if verbose: print(f\"Processing segment: {segment_name}\")\n",
    "\n",
    "    # Read segment labels\n",
    "    if verbose: print(\"Reading segment labels\")\n",
    "    frames_is_valid, kps_court, _, _, _ = read_segment_labels(\n",
    "        segment_path,\n",
    "        labels_path=labels_path,\n",
    "        load_frame_validity=True,\n",
    "        load_court=True,\n",
    "        load_ball=True,\n",
    "        load_player_bbox=True,\n",
    "    )\n",
    "\n",
    "    # Get matrices\n",
    "    if verbose: print(\"Get homography matrices\")\n",
    "    homography_matrices = [get_trans_matrix([list(point) if point is not None else None for point in points]) for points in kps_court]\n",
    "    homography_matrices = [cv2.invert(matrix_trans)[1] if matrix_trans is not None else None for matrix_trans in homography_matrices]\n",
    "    homography_time = time.time()\n",
    "    if verbose: print(f\"Done getting homography matrices ({homography_time - start_time} s)\")\n",
    "\n",
    "    # Read video\n",
    "    if verbose: print(\"Reading video\")\n",
    "    frames, fps =  read_video(segment_path)\n",
    "    frame_height, frame_width, _ = frames[0].shape\n",
    "    video_time = time.time()\n",
    "    # Filter valid frames\n",
    "    frames = [frame for frame, frame_is_valid in zip(frames, frames_is_valid) if frame_is_valid]\n",
    "    if verbose: print(f\"Done reading {len(frames)} video frames ({video_time - homography_time} s)\")\n",
    "\n",
    "    # Load models\n",
    "    if verbose: print(\"Loading models\")\n",
    "    person_detector = PersonDetector(device)\n",
    "    models_time = time.time()\n",
    "    if verbose: print(f\"Done loading models ({models_time - video_time} s)\")\n",
    "\n",
    "    # Detect people\n",
    "    if verbose: print(\"Detecting people\")\n",
    "    persons_top, persons_bottom = person_detector.track_players(frames[:3], homography_matrices, filter_players=False)\n",
    "    people_time = time.time()\n",
    "    if verbose: print(f\"Done detecting people ({people_time - models_time} s)\")\n",
    "\n",
    "    # Export labels\n",
    "    if verbose: print(\"Saving results\")\n",
    "    # Save players\n",
    "    player_bboxes = []\n",
    "    for is_top, (player_name, player_sequence) in enumerate(zip([\"btm\", \"top\"], [persons_bottom, persons_top])):\n",
    "        player_file = os.path.join(labels_path, f\"{segment_name}_player_{player_name}_bbox.npy\")\n",
    "        player_sequence = np.array([get_player_bboxes(kps_court[i], person_candidates, not is_top, frame_width=frame_width, frame_height=frame_height) for i, person_candidates in enumerate(player_sequence)])\n",
    "        player_bboxes.append(player_sequence)\n",
    "        #np.save(player_file, player_sequence)\n",
    "    save_time = time.time()\n",
    "    if verbose: print(f\"Total time: {save_time - start_time} s\")\n",
    "\n",
    "    return frames, player_bboxes[0], player_bboxes[1], kps_court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, bottom_bboxes, top_bboxes, kps_court = recompute_bboxes(\n",
    "    os.path.join(segments_path, \"V008_0038.mp4\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kps_court))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = len(bottom_bboxes)\n",
    "colors = [(255, 0, 0), (0, 0, 255)]\n",
    "for i in range(n_frames):\n",
    "    img = frames[i].copy()\n",
    "    \n",
    "    for j, bboxes in enumerate([top_bboxes, bottom_bboxes]):\n",
    "        for k, bbox in enumerate(bboxes):\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), colors[j], 2)\n",
    "\n",
    "    # Show img\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img[:,:,::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"avc1\")\n",
    "writer = cv2.VideoWriter(\"./video.mp4\", fourcc, 25, (1280, 720))\n",
    "\n",
    "# Visualize & write video\n",
    "colors = [(255, 0, 0), (0, 0, 255)]\n",
    "for i, frame in enumerate(frames):\n",
    "    # Skip invalid frames\n",
    "    img = frames[i].copy()\n",
    "    \n",
    "    for j, bboxes in enumerate([top_bboxes[i], bottom_bboxes[i]]):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        if x1 is None or y1 is None or x2 is None or y2 is None:\n",
    "            continue\n",
    "        img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), colors[j], 2)\n",
    "\n",
    "    writer.write(img)\n",
    "    frame_index += 1\n",
    "\n",
    "# Release video\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Player 2D Pose Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "from __init__ import data_path, models_path\n",
    "from data_utils import read_segment_frames, read_segment_labels, visualize_frame_annotations, visualize_segment_labels\n",
    "\n",
    "# Choose dataset\n",
    "dataset = \"tenniset\"\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "segments_path = os.path.join(dataset_path, \"segments\")\n",
    "labels_path = os.path.join(dataset_path, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read segment files\n",
    "segment_files = [file for file in sorted(os.listdir(segments_path)) if os.path.splitext(file)[-1] == \".mp4\"]\n",
    "print(f\"Number of segment files: {len(segment_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pose model\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "inferencer = MMPoseInferencer('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bbox_sequence(\n",
    "    bbox_sequence, \n",
    "    court_sequence, \n",
    "    is_btm,\n",
    "    derivative_threshold=5000,\n",
    "    make_plot=False,\n",
    "):\n",
    "    # Look at center points to gather inconsistencies\n",
    "    center_points = np.zeros((len(bbox_sequence), 2))\n",
    "    bbox_areas = np.zeros(len(bbox_sequence))\n",
    "    bbox_sequence_clean = np.copy(bbox_sequence)\n",
    "    missing_points = np.zeros(len(center_points), dtype=int)\n",
    "\n",
    "    # Extract center points wrt court from \n",
    "    for i, (bbox, court_points) in enumerate(zip(bbox_sequence, court_sequence)):\n",
    "        # Skip no bounding box detected\n",
    "        if np.any(bbox == None):\n",
    "            center_points[i, :] = np.inf\n",
    "            continue\n",
    "        xb1, yb1, xb2, yb2 = bbox\n",
    "        bbox_areas[i] = np.abs((xb2 - xb1) * (yb2 - yb1))\n",
    "\n",
    "        # Skip no court outline detected\n",
    "        court_outline = court_points[:4]\n",
    "        if np.any(court_outline == None):\n",
    "            center_points[i, :] = np.inf\n",
    "            continue\n",
    "        \n",
    "        # Get relevant center point of the court\n",
    "        (xtl, ytl), (xtr, ytr), (xbl, ybl), (xbr, ybr) = court_outline\n",
    "        x_ref = (xbl + xbr) / 2 if is_btm else (xtl + xtr) / 2\n",
    "        y_ref = (ybl + ybr) / 2 if is_btm else (ytl + ytr) / 2\n",
    "\n",
    "        # Get center point of the player's feet\n",
    "        x_player = (xb1 + xb2) / 2\n",
    "        y_player = yb2\n",
    "\n",
    "        # Save player center point referenced to court point\n",
    "        center_points[i, 0] = x_player - x_ref\n",
    "        center_points[i, 1] = y_player - y_ref\n",
    "\n",
    "    # Compute first derivative\n",
    "    center_points_derivative = np.vstack(([[0, 0]], center_points[:-1] - center_points[1:]))\n",
    "    center_points_derivative = center_points_derivative[:,0]**2 + center_points_derivative[:,1]**2\n",
    "    bbox_areas_derivative = np.abs(np.concatenate(([0], bbox_areas[:-1] - bbox_areas[1:])))\n",
    "\n",
    "    # Area jumps\n",
    "    bbox_area_jumps = np.sort(np.argwhere(bbox_areas_derivative > derivative_threshold).reshape(-1))\n",
    "    if len(bbox_area_jumps):\n",
    "        # print(\"JUMPS DETECTED\")\n",
    "        # print(bbox_area_jumps)\n",
    "        mean_area = np.mean(bbox_areas[:bbox_area_jumps[0]])\n",
    "    else:\n",
    "        mean_area = np.mean(bbox_areas)\n",
    "\n",
    "    # Determine jump points\n",
    "    jump_points = np.argwhere(np.logical_or(\n",
    "        center_points_derivative > derivative_threshold,\n",
    "        bbox_areas < mean_area / 2,\n",
    "    )).reshape(-1)\n",
    "    \n",
    "    # Return if no cleaning needs to be done\n",
    "    if len(jump_points) == 0:\n",
    "        return missing_points.astype(bool), bbox_sequence_clean\n",
    "\n",
    "    # Process jump points\n",
    "    indx_last = None\n",
    "    missing_start = False\n",
    "    for indx in jump_points:\n",
    "        #print(indx_last, indx)\n",
    "        if indx_last is None:\n",
    "            # First missing point\n",
    "            #print(\"FIRST MISSING POINT\")\n",
    "            missing_points[indx] = 1\n",
    "            missing_start = True\n",
    "        elif np.any(bbox_sequence[indx] == None):\n",
    "            # Missing point\n",
    "            #print(\"MISSING POINT\", indx)\n",
    "            missing_points[indx] = 1\n",
    "            missing_start = False\n",
    "        elif indx_last == indx - 1:\n",
    "            # Subsequent problematic points\n",
    "            #print(\"SUBSEQUENT MISSING POINT\")\n",
    "            missing_points[indx] = 1\n",
    "            missing_start = False\n",
    "        else:\n",
    "            # Distance between missing points\n",
    "            #print(\"DISTANCE BETWEEN MISSING POINTS\")\n",
    "            if missing_start:\n",
    "                # End point (hopefully)\n",
    "                missing_points[indx_last:indx+1] = 1\n",
    "                missing_start = False\n",
    "            else:\n",
    "                # Start point (hopefully)\n",
    "                missing_points[indx] = 1\n",
    "                missing_start = True\n",
    "\n",
    "        # Update last indx\n",
    "        indx_last = indx\n",
    "\n",
    "    # Fill gaps in missing points by linear interpolation\n",
    "    filled_center_points = np.copy(center_points)\n",
    "    missing_starts = np.argwhere((missing_points[1:] - missing_points[:-1]) == 1).reshape(-1)\n",
    "    missing_ends = np.argwhere((missing_points[1:] - missing_points[:-1]) == -1).reshape(-1)\n",
    "    for i, missing_start in enumerate(missing_starts):\n",
    "        # Get start value\n",
    "        if missing_start != 0:\n",
    "            # Previous value\n",
    "            cp_start_value = filled_center_points[missing_start-1]\n",
    "            bbox_start_value = bbox_sequence_clean[missing_start-1]\n",
    "        else:\n",
    "            # First valid value (TODO: fix if none is valid???)\n",
    "            cp_start_value = filled_center_points[not missing_points.astype(bool)][0]\n",
    "            bbox_start_value = bbox_sequence_clean[not missing_points.astype(bool)][0]\n",
    "\n",
    "        # Get missing end\n",
    "        if len(missing_ends) <= i:\n",
    "            # No matched endpoint - constant from startpoint onward\n",
    "            missing_end = len(filled_center_points) - 1\n",
    "            cp_end_value = cp_start_value\n",
    "            bbox_end_value = bbox_start_value\n",
    "        else:\n",
    "            # Get endpoint\n",
    "            missing_end = missing_ends[i]\n",
    "            cp_end_value = filled_center_points[missing_end]\n",
    "            bbox_end_value = bbox_sequence_clean[missing_end]\n",
    "            \n",
    "        # Linearly interpolate\n",
    "        n_points = missing_end - missing_start + 1\n",
    "        filled_center_points[missing_start:missing_end+1] = np.linspace(cp_start_value, cp_end_value, n_points)\n",
    "        bbox_sequence_clean[missing_start:missing_end+1] = np.linspace(bbox_start_value, bbox_end_value, n_points)\n",
    "\n",
    "    if make_plot:\n",
    "        # Plot results\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(center_points[:, 0], label=\"x\", color=\"tab:blue\")\n",
    "        ax.plot(center_points[:, 1], label=\"y\", color=\"tab:red\")\n",
    "        ax.plot(filled_center_points[:, 0], label=\"x filtered\", color=\"tab:cyan\")\n",
    "        ax.plot(filled_center_points[:, 1], label=\"y filtered\", color=\"tab:orange\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(\"Bottom Player\" if is_btm else \"Top Player\")\n",
    "        plt.show()\n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.plot(bbox_sequence[:, 0], label=\"x\", color=\"tab:blue\")\n",
    "        # ax.plot(bbox_sequence_clean[:, 0], label=\"x filtered\", color=\"tab:cyan\")\n",
    "        # ax.plot(bbox_sequence[:, 1], label=\"y\", color=\"tab:red\")\n",
    "        # ax.plot(bbox_sequence_clean[:, 1], label=\"y filtered\", color=\"tab:orange\")\n",
    "        # ax.grid(True)\n",
    "        # plt.show()\n",
    "\n",
    "        # Plot first derivative\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.plot(center_points_derivative, color=\"tab:blue\")\n",
    "        # ax.set_ylim(0, 2*derivative_threshold)\n",
    "        # ax.grid(True)\n",
    "        # ax.set_title(\"Bottom Player\" if is_btm else \"Top Player\")\n",
    "        # ax.axhline(derivative_threshold, color=\"tab:orange\")\n",
    "        # plt.show()\n",
    "    \n",
    "    return missing_points.astype(bool), bbox_sequence_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pose(\n",
    "    frame, \n",
    "    bbox, \n",
    "    crop_padding=50, \n",
    "    crop_img_width=256,\n",
    "    visualize_img=False,\n",
    "):\n",
    "    # Initialize empty keypoints\n",
    "    best_keypoints = np.array([[None, None]] * 17)\n",
    "    best_bbox = np.array([None, None, None, None])\n",
    "\n",
    "    # Frame size\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "\n",
    "    # Parse bounding box coords\n",
    "    if np.any(bbox == None):\n",
    "        return best_keypoints, best_bbox\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    xc, yc =  (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    w, h = abs(x2 - x1), abs(y2 - y1)\n",
    "    d = max(w, h) + crop_padding * 2\n",
    "\n",
    "    # Define cropping indices\n",
    "    x_crop1, x_crop2 = int(xc - d/2), int(xc + d/2)\n",
    "    y_crop1, y_crop2 = int(yc - d/2), int(yc + d/2)\n",
    "\n",
    "    # Make sure we don't crop past the edges of the frame\n",
    "    x_crop_offset = min(frame_width - x_crop2, max(-x_crop1, 0))\n",
    "    y_crop_offset = min(frame_height - y_crop2, max(-y_crop1, 0))\n",
    "    x_crop1 += x_crop_offset\n",
    "    x_crop2 += x_crop_offset\n",
    "    y_crop1 += y_crop_offset\n",
    "    y_crop2 += y_crop_offset\n",
    "    \n",
    "    # Crop image\n",
    "    img = frame[y_crop1:y_crop2,  x_crop1:x_crop2].copy()\n",
    "\n",
    "    # Resize img\n",
    "    scale = d / crop_img_width\n",
    "    img = cv2.resize(img, (crop_img_width, crop_img_width))\n",
    "\n",
    "    # Detect pose\n",
    "    result_generator = inferencer(img, show=False)\n",
    "    results = [result for result in result_generator]\n",
    "\n",
    "    # Keep best results\n",
    "    min_center_distance = np.inf\n",
    "    for result in results:\n",
    "        for prediction in result[\"predictions\"]:\n",
    "            for item in prediction:\n",
    "                # Parse item\n",
    "                pose_bbox = item[\"bbox\"]\n",
    "                keypoints = item[\"keypoints\"]\n",
    "\n",
    "                # Parse bbox\n",
    "                xb1, yb1, xb2, yb2 = pose_bbox[0]\n",
    "                center_x = (xb1 + xb2) / 2\n",
    "                center_y = (yb1 + yb2) / 2\n",
    "                center_distance = ((crop_img_width / 2 - center_x)**2  + (crop_img_width / 2 - center_y)**2)**(1/2)\n",
    "\n",
    "                # Keep track of best prediction\n",
    "                if center_distance < min_center_distance:\n",
    "                    min_center_distance = center_distance\n",
    "                    best_keypoints = keypoints\n",
    "                    best_bbox = pose_bbox[0]\n",
    "\n",
    "    # Visualize img\n",
    "    if visualize_img:\n",
    "        if not np.any(best_keypoints == None):\n",
    "            for i, keypoint in enumerate(best_keypoints):\n",
    "                cv2.circle(img, (int(keypoint[0]), int(keypoint[1])), radius=3, color=(150, 150, 50), thickness=-1)\n",
    "            x1, y1, x2, y2 = best_bbox\n",
    "            cv2.rectangle(\n",
    "                img, (int(x1), int(y1)), (int(x2), int(y2)), (150, 150, 50), 2\n",
    "            )\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img[:,:,::-1])\n",
    "        plt.show()\n",
    "    \n",
    "    # Rescale to frame reference\n",
    "    best_bbox = np.array(best_bbox)\n",
    "    best_keypoints = np.array(best_keypoints)\n",
    "    if not np.any(best_keypoints == None):\n",
    "        best_keypoints *= scale\n",
    "        best_keypoints[:, 0] += x_crop1\n",
    "        best_keypoints[:, 1] += y_crop1\n",
    "        best_bbox *= scale\n",
    "        best_bbox[0] += x_crop1\n",
    "        best_bbox[1] += y_crop1\n",
    "        best_bbox[2] += x_crop1\n",
    "        best_bbox[3] += y_crop1\n",
    "\n",
    "\n",
    "    return best_keypoints, best_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(\n",
    "    segment_path, \n",
    "    labels_path=labels_path,\n",
    "    crop_padding=50,\n",
    "    crop_width=224,\n",
    "    visualize_frame=False,\n",
    "):\n",
    "    # Load frames\n",
    "    segment_dir, segment_filename = os.path.split(segment_path)\n",
    "    segment_name, segment_ext = os.path.splitext(segment_filename)\n",
    "    frames, fps = read_segment_frames(segment_path, labels_path=labels_path, load_valid_frames_only=True)\n",
    "    if not len(frames):\n",
    "        return False\n",
    "\n",
    "    # Load labels\n",
    "    (\n",
    "        _,\n",
    "        court_sequence,\n",
    "        _,\n",
    "        player_btm_bbox_sequence,\n",
    "        player_top_bbox_sequence,\n",
    "        _,\n",
    "        _,\n",
    "    ) = read_segment_labels(\n",
    "        segment_path, \n",
    "        labels_path=labels_path,\n",
    "        load_frame_validity=True,\n",
    "        load_court=True,\n",
    "        load_ball=False,\n",
    "        load_player_bbox=True,\n",
    "        load_player_pose=False,\n",
    "        use_pose_bbox=False,\n",
    "    )\n",
    "\n",
    "    btm_missing_points, btm_bbox_clean = clean_bbox_sequence(\n",
    "        player_btm_bbox_sequence,\n",
    "        court_sequence,\n",
    "        is_btm=True,\n",
    "        make_plot=True,\n",
    "    )\n",
    "    top_missing_points, top_bbox_clean = clean_bbox_sequence(\n",
    "        player_top_bbox_sequence,\n",
    "        court_sequence,\n",
    "        is_btm=False,\n",
    "        make_plot=True,\n",
    "    )\n",
    "\n",
    "    # Process frames\n",
    "    players_bbox_last = [None, None]\n",
    "    players_bbox_sequences = [[None] *  len(frames) , [None] * len(frames)]\n",
    "    players_pose_sequences = [[None] *  len(frames) , [None] * len(frames)]\n",
    "    for frame_index, frame in tqdm(enumerate(frames)):\n",
    "        # Get frame labels\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        players_bbox = [player_top_bbox_sequence[frame_index], player_btm_bbox_sequence[frame_index]]\n",
    "        players_bbox_clean = [top_bbox_clean[frame_index], btm_bbox_clean[frame_index]]\n",
    "        players_missing = [top_missing_points[frame_index], btm_missing_points[frame_index]]\n",
    "\n",
    "        # Perform pose detection\n",
    "        for is_btm, bbox in enumerate(players_bbox):\n",
    "            if players_missing[is_btm]:\n",
    "                # Try to recover player pose from best knowledge\n",
    "                for i, bbox_candidate in enumerate([players_bbox_last[is_btm], players_bbox_clean[is_btm], players_bbox[is_btm]]):\n",
    "                    # Skip invalid bboxes\n",
    "                    if bbox_candidate is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Detect pose\n",
    "                    pose_keypoints, pose_bbox = detect_pose(\n",
    "                        frame, \n",
    "                        bbox_candidate, \n",
    "                        crop_padding=crop_padding, \n",
    "                        crop_img_width=crop_width,\n",
    "                        visualize_img=False,\n",
    "                    )\n",
    "\n",
    "                    # Break if result is valid\n",
    "                    if not np.any(pose_keypoints == None):\n",
    "                        break\n",
    "            else:\n",
    "                # Detect pose\n",
    "                pose_keypoints, pose_bbox = detect_pose(\n",
    "                    frame, \n",
    "                    bbox, \n",
    "                    crop_padding=crop_padding, \n",
    "                    crop_img_width=crop_width,\n",
    "                    visualize_img=False,\n",
    "                )\n",
    "\n",
    "            # Update last bbox\n",
    "            players_bbox_last[is_btm] = pose_bbox\n",
    "\n",
    "            # Save bounding box & pose\n",
    "            players_bbox_sequences[is_btm][frame_index] = pose_bbox\n",
    "            players_pose_sequences[is_btm][frame_index] = pose_keypoints\n",
    "\n",
    "            # Visualize on frame\n",
    "            if visualize_frame and not np.any(pose_keypoints == None):\n",
    "                for keypoint in pose_keypoints:\n",
    "                    cv2.circle(frame, (int(keypoint[0]), int(keypoint[1])), radius=5, color=(150, 150, 50), thickness=-1)\n",
    "                x1, y1, x2, y2 = pose_bbox\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (150, 150, 50), 2)\n",
    "                    \n",
    "\n",
    "        # Visualize frame\n",
    "        if visualize_frame:\n",
    "            img = visualize_frame_annotations(\n",
    "                frame, \n",
    "                court_sequence[frame_index], \n",
    "                None, \n",
    "                players_bbox[1], \n",
    "                players_bbox[0], \n",
    "                show_court_numbers=False, \n",
    "                show_img=True,\n",
    "            )\n",
    "\n",
    "    # Export labels\n",
    "    for is_btm in range(2):\n",
    "        player_name = \"btm\" if is_btm else \"top\"\n",
    "        player_bbox_file = os.path.join(labels_path, f\"{segment_name}_player_{player_name}_bbox_pose.npy\")\n",
    "        player_pose_file = os.path.join(labels_path, f\"{segment_name}_player_{player_name}_pose.npy\")\n",
    "        np.save(player_bbox_file, players_bbox_sequences[is_btm])\n",
    "        np.save(player_pose_file, players_pose_sequences[is_btm])\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = process_segment(os.path.join(segments_path, \"V009_0061.mp4\"), visualize_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "from __init__ import data_path, models_path\n",
    "from data_utils import read_segment_frames, read_segment_labels, visualize_frame_annotations, visualize_segment_labels\n",
    "\n",
    "# Choose dataset\n",
    "dataset = \"tenniset\"\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "segments_path = os.path.join(dataset_path, \"segments\")\n",
    "labels_path = os.path.join(dataset_path, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_segment_labels(os.path.join(segments_path, \"V009_0061.mp4\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment_file in tqdm(segment_files):\n",
    "    segment_name, _ = os.path.splitext(segment_file)\n",
    "    if not os.path.exists(os.path.join(labels_path, f\"{segment_name}_player_btm_pose.npy\")):\n",
    "        continue\n",
    "    out_path = visualize_segment_labels(os.path.join(segments_path, segment_file), overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
